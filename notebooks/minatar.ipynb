{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from rgm import *\n",
    "from atari.common import *\n",
    "\n",
    "import mediapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_game(game_config(\"Breakout\", ObservationType.MINI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 11673.80it/s]\n",
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got reward (avg/max) 0.395 4\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "observations = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "num_episodes = 1000\n",
    "horizon = 128\n",
    "\n",
    "for n in tqdm(range(num_episodes)):\n",
    "    acs = []\n",
    "    os = []\n",
    "    rs = []\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    for i in range(horizon):\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, done, trunc, info  = env.step(action)\n",
    "        acs.append([action])\n",
    "        os.append(obs)\n",
    "        rs.append(reward)\n",
    "        obs = next_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            obs, info = env.reset()\n",
    "\n",
    "    observations.append(os)\n",
    "    actions.append(acs)\n",
    "    rewards.append(rs)\n",
    "\n",
    "totals = []\n",
    "for r in rewards:\n",
    "    totals.append(jnp.sum(jnp.asarray(r)))\n",
    "print(\"Got reward (avg/max)\", jnp.mean(jnp.asarray(totals)), jnp.max(jnp.asarray(totals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "\n",
    "def render(observation):\n",
    "    observation = observation[:, :, :4]\n",
    "    cmap = sns.color_palette(\"cubehelix\", observation.shape[-1])\n",
    "    cmap.insert(0, (0, 0, 0))\n",
    "    cmap = colors.ListedColormap(cmap)\n",
    "    bounds = [i for i in range(observation.shape[-1] + 2)]\n",
    "    norm = colors.BoundaryNorm(bounds, observation.shape[-1] + 1)\n",
    "    numerical_state = np.amax(observation * np.reshape(np.arange(observation.shape[-1]) + 1, (1, 1, -1)), 2) + 0.5\n",
    "    img = cmap(norm(numerical_state))\n",
    "    return img[:, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one-hot dimension for background\n",
    "def to_one_hot(o):\n",
    "    zero_mask = jnp.all(o == 0, axis=-1)  # shape of batch x time x 10 x 10\n",
    "    arr = jnp.zeros((o.shape[0], o.shape[1], o.shape[2], o.shape[3], o.shape[4] + 1), dtype=jnp.float32)\n",
    "    arr = arr.at[..., :4].set(o)\n",
    "    arr = arr.at[zero_mask, 4].set(1)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "env = make_game(game_config(\"Asterix\", ObservationType.MINI))\n",
    "env.unwrapped.sticky_action_prob = 0.0\n",
    "\n",
    "steps = 0\n",
    "last_step = 0\n",
    "train_steps = 0\n",
    "\n",
    "observations = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "train_sequences = {}\n",
    "\n",
    "num_episodes = 0\n",
    "horizon = 128\n",
    "\n",
    "rgm = RGM(max_levels=8, n_bins=5, dx=2, size=(10, 10), action_range=(0, 4), svd=False)\n",
    "rgm_agent = RGMAgent(rgm)\n",
    "\n",
    "\n",
    "# for n in tqdm(range(num_episodes)):\n",
    "while num_episodes < 20:\n",
    "    print(steps,\"\\t\", steps - last_step)\n",
    "    last_step = steps\n",
    "\n",
    "    acs = []\n",
    "    os = []\n",
    "    rs = []\n",
    "    \n",
    "    num_episodes += 1\n",
    "    rgm_agent.reset()\n",
    "    obs, info = env.reset()\n",
    "    for i in range(horizon):\n",
    "        o = to_one_hot(jnp.asarray([[obs]]))[0].reshape(-1, 100, 5).transpose(1, 0, 2)\n",
    "        action = rgm_agent.act(o)\n",
    "        action = int(action[0])\n",
    "        if action == -1:\n",
    "            # if rgm_agent returns -1, randomly sample an action\n",
    "            action = env.action_space.sample()\n",
    "        next_obs, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        acs.append([action])\n",
    "        os.append(obs)\n",
    "        rs.append([reward])\n",
    "        obs = next_obs\n",
    "\n",
    "        if done:\n",
    "            achieved_reward = jnp.asarray(rs)\n",
    "            r = jnp.where(achieved_reward > 0)[0]\n",
    "            if len(r) > 0:\n",
    "                last_reward_idx = r[-1]\n",
    "                step_length = 8\n",
    "                size = step_length * (1 + last_reward_idx // step_length)\n",
    "                if len(os) >= size:\n",
    "                    imgs = []\n",
    "                    for j in range(len(os)):\n",
    "                        imgs.append(render(os[j]))\n",
    "                    train_sequences[str(len(train_sequences.keys()))] = imgs\n",
    "\n",
    "                    train_steps += size\n",
    "                    # we have a trajectory with some rewards, add to RGM?\n",
    "                    o = to_one_hot(jnp.asarray([os]))[0].reshape(-1, 100, 5).transpose(1, 0, 2)\n",
    "                    # lump reward on there as an \"action\" modality to use for acting\n",
    "                    a = jnp.concatenate([jnp.asarray(acs), jnp.asarray(rs)], axis=-1)\n",
    "                    rgm.learn_structure(o, a)\n",
    "            break            \n",
    "\n",
    "    # only train on 1 sequence for debugging\n",
    "    # if rgm.agents is not None:\n",
    "    #     break\n",
    "\n",
    "    observations.append(os)\n",
    "    actions.append(acs)\n",
    "    rewards.append(rs)\n",
    "\n",
    "    jax.clear_caches()\n",
    "\n",
    "\n",
    "print(\"Interacted with the environment for\", steps, \"steps\")\n",
    "\n",
    "totals = []\n",
    "for r in rewards:\n",
    "    totals.append(jnp.sum(jnp.asarray(r)))\n",
    "print(\"Got reward (avg/max)\", jnp.mean(jnp.asarray(totals)), jnp.max(jnp.asarray(totals)))\n",
    "print(\"Trained on\", len(train_sequences.keys()), \"sequences, totalling\", train_steps, \"steps\")\n",
    "\n",
    "with mediapy.set_show_save_dir(\"/tmp\"):\n",
    "    mediapy.show_videos(train_sequences, width=160, height=160, columns=5, fps=4, codec=\"gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "resize = lambda x: jnp.array(Image.fromarray((x * 255).astype(jnp.uint8)).resize((256, 256), Image.NEAREST))\n",
    "train_sequences_big = {k: [resize(img) for img in imgs] for k, imgs in train_sequences.items()}\n",
    "\n",
    "with mediapy.set_show_save_dir(\"/tmp\"):\n",
    "    mediapy.show_videos(train_sequences_big, width=160, height=160, columns=5, fps=6, codec=\"gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapy\n",
    "\n",
    "imgs = []\n",
    "for i in range(len(observations)):\n",
    "    for j in range(len(observations[i])):\n",
    "        imgs.append(render(observations[i][j]))\n",
    "\n",
    "from PIL import Image\n",
    "resize = lambda x: jnp.array(Image.fromarray((x * 255).astype(jnp.uint8)).resize((256, 256), Image.NEAREST))\n",
    "big = [resize(img) for img in imgs]\n",
    "\n",
    "with mediapy.set_show_save_dir(\"/tmp\"):\n",
    "    mediapy.show_videos({\"breakout\": big}, width=320, height=320, fps=20, codec=\"gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgm.save(\"../data/rgms/mini_breakout_rgm.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "play = []\n",
    "actions = []\n",
    "\n",
    "n = 1\n",
    "for _ in tqdm(range(n)):\n",
    "    total_reward = 0\n",
    "\n",
    "    rgm_agent = RGMAgent(rgm)\n",
    "    rgm_agent.reset()\n",
    "    obs, info = env.reset()\n",
    "    for i in range(horizon):\n",
    "        o = to_one_hot(jnp.asarray([[obs]]))[0].reshape(-1, 100, 5).transpose(1, 0, 2)\n",
    "        action = rgm_agent.act(o)\n",
    "        action = int(action[0])\n",
    "        if action == -1:\n",
    "            print(\"random\")\n",
    "            # if rgm_agent returns -1, randomly sample an action\n",
    "            action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "        next_obs, reward, done, trunc, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        play.append(obs)\n",
    "        obs = next_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "print(\n",
    "    \"Played \", n, \" games with reward (avg / max)\",\n",
    "    jnp.mean(jnp.asarray(episode_rewards)),\n",
    "    jnp.max(jnp.asarray(episode_rewards)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapy\n",
    "\n",
    "imgs = [render(play[i]) for i in range(len(play))]\n",
    "\n",
    "\n",
    "with mediapy.set_show_save_dir(\"/tmp\"):\n",
    "    mediapy.show_videos({\"breakout\": imgs}, width=320, height=320, fps=2, codec=\"gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
